# data_analyzer.py
import pandas as pd
from datetime import datetime
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import warnings
import logging # Import logging module ğŸ“
import jdatetime

# Setup a logger for this module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Set logging level for this module

# Suppress KMeans warning for n_init in older scikit-learn versions ğŸ¤«
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn.cluster._kmeans")

def calculate_rfm(df_transactions):
    """
    Calculates RFM (Recency, Frequency, Monetary) values for each customer
    based on their transaction data. ğŸ“Š

    Args:
        df_transactions (pd.DataFrame): DataFrame containing transaction data
                                       Ø¨Ø§ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'Ø´Ù†Ø§Ø³Ù‡ Ù…Ø´ØªØ±ÛŒ', 'ØªØ§Ø±ÛŒØ® ÙØ§Ú©ØªÙˆØ±', 'Ù…Ø¨Ù„Øº (ØªÙˆÙ…Ø§Ù†)'. ğŸ“ˆ

    Returns:
        pd.DataFrame: A DataFrame with 'Ø´Ù†Ø§Ø³Ù‡ Ù…Ø´ØªØ±ÛŒ', 'Recency', 'Frequency', 'Monetary' columns.
                      Returns an empty DataFrame if input is empty or dates are invalid. ğŸš«
    """
    if df_transactions.empty:
        return pd.DataFrame()

    # Convert 'ØªØ§Ø±ÛŒØ® ÙØ§Ú©ØªÙˆØ±' to datetime objects directly using pandas.to_datetime.
    # This is robust for various standard Gregorian date formats.
    # 'errors='coerce' will turn any unparseable dates into NaT (Not a Time).
    def convert_shamsi_to_gregorian(shamsi_str):
        try:
            y, m, d = map(int, str(shamsi_str).split('-'))
            return jdatetime.date(y, m, d).togregorian()
        except:
            return pd.NaT

    df_transactions['ØªØ§Ø±ÛŒØ® ÙØ§Ú©ØªÙˆØ±_greg'] = df_transactions['ØªØ§Ø±ÛŒØ® ÙØ§Ú©ØªÙˆØ±'].apply(convert_shamsi_to_gregorian)

    
    # Drop rows where date conversion resulted in NaT (Not a Time)
    df_transactions.dropna(subset=['ØªØ§Ø±ÛŒØ® ÙØ§Ú©ØªÙˆØ±_greg'], inplace=True) 

    if df_transactions['ØªØ§Ø±ÛŒØ® ÙØ§Ú©ØªÙˆØ±_greg'].empty:
        return pd.DataFrame() # No valid transactions to calculate RFM ğŸ¤·â€â™‚ï¸

    # Define a snapshot date as the day after the last transaction date (Gregorian) ğŸ—“ï¸
    # This ensures Recency is calculated correctly for the most recent purchase
    snapshot_date = df_transactions['ØªØ§Ø±ÛŒØ® ÙØ§Ú©ØªÙˆØ±_greg'].max() + pd.Timedelta(days=1)
    
    # Calculate RFM components â•
    rfm_df = df_transactions.groupby('Ø´Ù†Ø§Ø³Ù‡ Ù…Ø´ØªØ±ÛŒ').agg(
        Recency=('ØªØ§Ø±ÛŒØ® ÙØ§Ú©ØªÙˆØ±_greg', lambda date: (snapshot_date - date.max()).days), # Days since last purchase â°
        Frequency=('Ø´Ù…Ø§Ø±Ù‡ ÙØ§Ú©ØªÙˆØ±', 'count'), # Count of transactions ğŸ”¢
        Monetary=('Ù…Ø¨Ù„Øº (ØªÙˆÙ…Ø§Ù†)', 'sum') # Total spending ğŸ’²
    ).reset_index()

    return rfm_df

def segment_customers_kmeans(rfm_df, n_clusters=3):
    """
    Segments customers into clusters using KMeans algorithm based on their RFM values. ğŸ§‘â€ğŸ¤â€ğŸ§‘
    Standardizes RFM features before clustering. ğŸ“

    Args:
        rfm_df (pd.DataFrame): DataFrame with 'Ø´Ù†Ø§Ø³Ù‡ Ù…Ø´ØªØ±ÛŒ', 'Recency', 'Frequency', 'Monetary'. ğŸ“Š
        n_clusters (int): The number of clusters to form. ğŸ”¢

    Returns:
        pd.DataFrame: The input rfm_df with an additional 'Segment' column indicating the cluster. ğŸ·ï¸
                      Returns the original rfm_df if clustering cannot be performed (e.g., not enough data). ğŸš«
    """
    if rfm_df.empty:
        return rfm_df

    # Features for clustering ğŸ§©
    X = rfm_df[['Recency', 'Frequency', 'Monetary']]

    # Handle cases where n_clusters is greater than the number of samples âš ï¸
    if len(X) < n_clusters:
        # Adjust n_clusters to be at most the number of samples â¬‡ï¸
        n_clusters = len(X)
        if n_clusters == 0:
            return rfm_df # No data to cluster ğŸ¤·â€â™€ï¸
        if n_clusters < 2:
            # If only one or zero samples, clustering is trivial or impossible ğŸ›‘
            rfm_df['Segment'] = 0 # Assign all to segment 0 (single cluster) ğŸ¯
            return rfm_df

    # Standardize the RFM features. This is crucial for KMeans as it's distance-based. ğŸ“
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # Apply KMeans clustering ğŸ§ 
    # n_init='auto' handles the warning for explicit initialization in newer scikit-learn versions
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')
    rfm_df['Segment'] = kmeans.fit_predict(X_scaled) # Predict on scaled data âœ…

    return rfm_df

def perform_analysis(df_transactions, df_customers): # Added df_customers parameter
    """
    Performs comprehensive customer analysis using RFM and KMeans clustering. ğŸ“Š
    Generates a human-readable report summarizing customer segments and overall insights. ğŸ“

    Args:
        df_transactions (pd.DataFrame): DataFrame containing transaction data. ğŸ“ˆ
        df_customers (pd.DataFrame): DataFrame containing customer data (for names). ğŸ§‘â€ğŸ¤â€ğŸ§‘

    Returns:
        str: A formatted string containing the analysis report. ğŸ“„
    """
    rfm_df = calculate_rfm(df_transactions)

    if rfm_df.empty:
        return "Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„ RFM ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯. ğŸ˜”"

    # Segment customers into 3 clusters (you can adjust n_clusters based on your data) ğŸ§©
    segmented_rfm = segment_customers_kmeans(rfm_df, n_clusters=3)

    if segmented_rfm.empty:
        return "Ø§Ù…Ú©Ø§Ù† Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø´ØªØ±ÛŒØ§Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´Øª. ğŸš«"

    report_content = "--- Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ù…Ø´ØªØ±ÛŒØ§Ù† ğŸ“Š ---\n\n"

    # Analyze and describe each segment
    # Sort segments by their average Monetary value in descending order ğŸ’°
    segment_summary = segmented_rfm.groupby('Segment').agg(
        Avg_Recency=('Recency', 'mean'),
        Avg_Frequency=('Frequency', 'mean'),
        Avg_Monetary=('Monetary', 'mean'),
        Num_Customers=('Ø´Ù†Ø§Ø³Ù‡ Ù…Ø´ØªØ±ÛŒ', 'count')
    ).sort_values(by='Avg_Monetary', ascending=False).reset_index()

    # Define descriptive labels for segments based on their sorted order (by Monetary value)
    # These labels provide more meaningful context to the user.
    descriptive_labels = [
        "Ø¨Ø®Ø´ Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§Ø§Ø±Ø²Ø´ (Ù‚Ù‡Ø±Ù…Ø§Ù†Ø§Ù† ğŸ†)",
        "Ø¨Ø®Ø´ Ù…Ø´ØªØ±ÛŒØ§Ù† ÙØ¹Ø§Ù„ (Ø¨Ø§ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø±Ø´Ø¯ ğŸŒ±)",
        "Ø¨Ø®Ø´ Ù…Ø´ØªØ±ÛŒØ§Ù† Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ ØªÙˆØ¬Ù‡ (Ø¯Ø± Ù…Ø¹Ø±Ø¶ Ø®Ø·Ø± âš ï¸)"
    ]
    
    # Handle cases where fewer than 3 clusters are formed
    if len(segment_summary) < len(descriptive_labels):
        # Adjust labels if fewer clusters are present
        adjusted_labels = descriptive_labels[:len(segment_summary)]
    else:
        adjusted_labels = descriptive_labels

    # Merge segmented_rfm with df_customers to get customer names
    # Ensure 'Ø´Ù†Ø§Ø³Ù‡ Ù…Ø´ØªØ±ÛŒ' in segmented_rfm corresponds to 'Ú©Ø¯ Ù…Ø´ØªØ±ÛŒ' in df_customers
    # Using 'left' merge to keep all customers from segmented_rfm
    segmented_rfm_with_names = pd.merge(
        segmented_rfm,
        df_customers[['Ú©Ø¯ Ù…Ø´ØªØ±ÛŒ', 'Ù†Ø§Ù…']],
        left_on='Ø´Ù†Ø§Ø³Ù‡ Ù…Ø´ØªØ±ÛŒ',
        right_on='Ú©Ø¯ Ù…Ø´ØªØ±ÛŒ',
        how='left'
    )

    # Group by segment and collect customer names for each segment
    # Using .unique() to avoid duplicate names if a customer appears multiple times in a segment (though unlikely with RFM)
    segment_customer_names = segmented_rfm_with_names.groupby('Segment')['Ù†Ø§Ù…'].apply(lambda x: list(x.unique())).to_dict()


    for index, row in segment_summary.iterrows():
        segment_id = int(row['Segment'])
        avg_recency = row['Avg_Recency']
        avg_frequency = row['Avg_Frequency']
        avg_monetary = row['Avg_Monetary']
        num_customers = row['Num_Customers']
        
        # Assign a more descriptive label based on the sorted order (index)
        current_segment_label = adjusted_labels[index] if index < len(adjusted_labels) else f"Ø¨Ø®Ø´ {segment_id + 1}"

        report_content += f"{current_segment_label} (ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´ØªØ±ÛŒØ§Ù†: {num_customers}):\n"
        
        # Get customer names for this specific segment
        customers_in_this_segment = segment_customer_names.get(segment_id, [])
        if customers_in_this_segment:
            customer_names_str = ", ".join(customers_in_this_segment)
            report_content += f"  Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´: {customer_names_str}\n" # New line for customer names
        else:
            report_content += "  Ù…Ø´ØªØ±ÛŒØ§Ù†ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ ÛŒØ§ÙØª Ù†Ø´Ø¯Ù†Ø¯. ğŸ¤·\n"
        
        report_content += f"  Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªØ§Ø²Ú¯ÛŒ (Ø±ÙˆØ² Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ø®Ø±ÛŒØ¯): {avg_recency:.0f} Ø±ÙˆØ² â°\n"
        report_content += f"  Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ØªÚ©Ø±Ø§Ø± (ØªØ¹Ø¯Ø§Ø¯ Ø®Ø±ÛŒØ¯): {avg_frequency:.1f}\n"
        report_content += f"  Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ø±Ø²Ø´ Ù¾ÙˆÙ„ÛŒ (Ú©Ù„ Ù‡Ø²ÛŒÙ†Ù‡): {avg_monetary:,.0f} ØªÙˆÙ…Ø§Ù† ğŸ’²\n"
        
        # Provide interpretations based on the assigned label and general RFM characteristics
        if "Ø¨Ø§Ø§Ø±Ø²Ø´" in current_segment_label:
            report_content += "  Ø§ÛŒÙ† Ù…Ø´ØªØ±ÛŒØ§Ù† Ø§Ø®ÛŒØ±Ø§Ù‹ Ø²ÛŒØ§Ø¯ Ø®Ø±ÛŒØ¯ Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§ØºÙ„Ø¨ ØªÚ©Ø±Ø§Ø± Ø®Ø±ÛŒØ¯ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù‡Ø²ÛŒÙ†Ù‡ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø­ÙØ¸ Ø§ÛŒÙ† Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø±Ø§ÛŒ Ú©Ø³Ø¨â€ŒÙˆÚ©Ø§Ø± Ø´Ù…Ø§ Ø­ÛŒØ§ØªÛŒ Ø§Ø³Øª. ğŸ’\n"
        elif "ÙØ¹Ø§Ù„" in current_segment_label:
            report_content += "  Ø§ÛŒÙ† Ù…Ø´ØªØ±ÛŒØ§Ù† Ù†Ø³Ø¨ØªØ§Ù‹ ÙØ¹Ø§Ù„ Ù‡Ø³ØªÙ†Ø¯ Ùˆ Ù¾ØªØ§Ù†Ø³ÛŒÙ„ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø´Ø¯Ù† Ø¨Ù‡ Ù…Ø´ØªØ±ÛŒØ§Ù† Ø¨Ø§Ø§Ø±Ø²Ø´ Ø¯Ø§Ø±Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ù‡Ø§ÛŒ Ø¬Ø°Ø§Ø¨ ØªØ´ÙˆÛŒÙ‚ Ø¨Ù‡ Ø®Ø±ÛŒØ¯Ù‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ú©Ù†ÛŒØ¯. âœ¨\n"
        elif "Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ ØªÙˆØ¬Ù‡" in current_segment_label:
            report_content += "  Ø§ÛŒÙ† Ù…Ø´ØªØ±ÛŒØ§Ù† Ù…Ø¯ØªÛŒ Ø§Ø³Øª Ú©Ù‡ Ø®Ø±ÛŒØ¯ Ù†Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯ ÛŒØ§ Ú©Ù…ØªØ± ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù‡â€ŒØ§Ù†Ø¯. Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± Ù…Ø¹Ø±Ø¶ Ø®Ø·Ø± Ø±ÛŒØ²Ø´ Ø¨Ø§Ø´Ù†Ø¯. Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¢Ù†â€ŒÙ‡Ø§ØŒ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ø§ØµÛŒ (Ù…Ø§Ù†Ù†Ø¯ ØªØ®ÙÛŒÙ ÛŒØ§ ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ) Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯. ğŸ”™\n"
        else:
            report_content += "  Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ø§ØµÛŒ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ± Ø¯Ø§Ø±Ø¯. ğŸ¤”\n"
        
        report_content += "\n"

    # Overall summary ğŸ“ˆ
    total_customers_analyzed = len(rfm_df)
    total_transactions_processed = len(df_transactions)
    total_sales_volume = df_transactions['Ù…Ø¨Ù„Øº (ØªÙˆÙ…Ø§Ù†)'].sum()
    
    report_content += f"--- Ø®Ù„Ø§ØµÙ‡ Ú©Ù„ÛŒ ğŸ“Š ---\n"
    report_content += f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø´ØªØ±ÛŒØ§Ù† ØªØ­Ù„ÛŒÙ„ Ø´Ø¯Ù‡: {total_customers_analyzed} ğŸ§‘â€ğŸ¤â€ğŸ§‘\n"
    report_content += f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ØªØ±Ø§Ú©Ù†Ø´â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡: {total_transactions_processed} ï¿½\n"
    report_content += f"Ø­Ø¬Ù… Ú©Ù„ ÙØ±ÙˆØ´: {total_sales_volume:,.0f} ØªÙˆÙ…Ø§Ù† ğŸ’°\n"
    report_content += "\nØ§ÛŒÙ† ØªØ­Ù„ÛŒÙ„ Ø¨Ù‡ Ø´Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø§Ø³ØªØ±Ø§ØªÚ˜ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²Ø§Ø±ÛŒØ§Ø¨ÛŒ Ùˆ ÙØ±ÙˆØ´ Ø®ÙˆØ¯ Ø±Ø§ Ù‡Ø¯ÙÙ…Ù†Ø¯ØªØ± Ú©Ù†ÛŒØ¯. âœ¨"

    return report_content